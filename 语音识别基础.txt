asrt核心逻辑：
    1、通过声学模型【采用深度卷积神经网络【DCNN】和连接时序分类【CTC】方法】，将声音转录为中文拼音；
    2、通过语言模型，将拼音序列转换为中文文本。

特征提取：
    将普通的wav语音信号通过分帧加窗等操作转换为神经网络需要的二维频谱图像信号，即语谱图。

模型：
    1、声学模型：DCNN+CTC。
    作用：用于实现将声学信号转换为拼音标签序列。
    工程：基于keras和tensorflow框架
    CTC解码：在声学模型的输出中，往往包含了大量连续重复的符号，因此需要将连续相同的符号合并为同一个符号，然后再去除静音分割标记符，得到最终实际的语音拼音符号序列

    2、语言模型：基于概率图的马尔可夫模型。
    作用：用于实现将拼音标签序列转换为最终对应的中文文本【即识别出来的文字】。拼音转文本的本质被建模为一条隐含马尔科夫链。

训练过程：
    0、定义声学模型【内含神经网络和损失函数】和优化器，获取语音模型
    1、加载训练数据
    2、编译调试损失函数和优化器
    3、对于输入数据进行特征提取
    4、计算单次迭代的单批训练的数据量【总量/批数】
    5、分迭代进行训练【每次迭代内，分批次训练。训练之前的输入数据有进行特征提取吗？】并保存具备迭代号标记的训练模型【保存模型方法只传入了文件名，模型数据在哪里呢？利用了tensorflow中的Model类，保存模型时只需传入文件名即可。一般保存模型保存的是训练得到的最佳【准确率最高或者误差最小】网络权重参数】
    6、保存最终训练模型

预测/识别过程：
    0、加载训练好的语音模型
    1、读取wav音频文件，输出矩阵语音数据
    2、对语音矩阵提取特征
    3、利用语音模型预测特征得出拼音结果索引
    4、加载字典拼音数据，依据索引获取最终识别的拼音结果
    5、利用语音模型将拼音结果翻译为中文

总结：
    1、训练和预测/识别阶段共性：对于输入音频数据进行同样的特征提取及预处理
    2、训练和预测/识别阶段不同之处：训练依据模型及优化器和输入特征及标签得出网络模型；识别依据训练好的模型进行识别预测

训练过程：
训练数据加载过程：见data_loader.py文件_load_data方法代码注释
特征提取和数据整形：见speech_features.py中类Spectrogram的run方法代码注释【特征算法处理，待学习】和speech_model.py文件_data_generator方法代码注释
分迭代【迭代内分批次】训练和保存模型：见speech_model.py文件train_model方法代码注释
保存模型【提问：分迭代保存模型和训练后保存模型有什么联系？为啥要在两个地方出现保存模型？】

评估过程：
评估数据加载过程：见data_loader.py文件_load_data方法代码注释
随机选取音频文件，进行模型预测：
    特征提取和数据整形：见speech_features.py中类Spectrogram的run方法代码注释【特征算法处理，待学习】和speech_model.py文件_data_generator方法代码注释
    模型预测：见keras_backend.py中类SpeechModel251BN的forward方法代码注释【预测数据，待学习】
    计算误差：见ops.py中的get_edit_distance方法代码注释【计算编辑距离，待学习】
    打印或保存误差及错误率信息

识别过程：
加载训练好的模型：利用tensorflow中的Model类load_weights方法
读取输入音频文件：读取单个音频文件
特征提取和数据整形：见speech_features.py中类Spectrogram的run方法代码注释【特征算法处理，待学习】和speech_model.py文件_data_generator方法代码注释
模型预测：见keras_backend.py中类SpeechModel251BN的forward方法代码注释【预测数据，待学习】
获取拼音结果：将预测得到的拼音索引依赖拼音列表获取拼音结果
将拼音翻译为中文：利用N-Gram语言模型将拼音列表翻译为中文，见language_model3.py中的类ModelLanguage的pinyin_to_text方法
